> Gemini Prompt:

Use the article list from task t000_dr02_article-list (added below this prompt) as the seed base.

Perform a deep research expansion focused on identifying:
- Newer publications (2024–2025) that **build upon or challenge** the seed papers
- **Interconnected breakthroughs** not yet referenced (e.g., newer variants of ViT, improvements to ArcFace, cross-modal foundation models)
- **Emerging research frontiers** in underrepresented areas (e.g., multimodal RL, AI safety, scientific reasoning with LLMs)

For each discovery, include:
1. Title
2. Authors
3. Year
4. Core contribution (1–3 lines)
5. Domain category (NLP, CV, Face Recognition, RL, etc.)
6. Link (arXiv, GitHub, or official)

Organize results by domain. Focus on recency, relevance, and novelty. Prioritize cross-domain models and paradigm shifts.

> Gemini Seed:

Based on the comprehensive historical analysis conducted, here is a categorized list of influential articles, foundational works, and key concepts that represent significant milestones in the evolution of machine learning, with an emphasis on face recognition and deep learning, now updated with recent research:
Early Foundational Milestones & Concepts
1.	Computing Machinery and Intelligence (Alan Turing, 1950) - Introduced the "Turing Test" as a measure of machine intelligence.
2.	The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain (Frank Rosenblatt, 1958) - Introduced the Perceptron algorithm.
3.	A Logical Calculus of the Ideas Immanent in Nervous Activity (McCulloch & Pitts, 1943) - Proposed early mathematical models of neurons.
4.	Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position (Kunihiko Fukushima, 1980) - A precursor to Convolutional Neural Networks.
Pre-Deep Learning Era: Key Algorithms & Techniques
5.	Eigenfaces for Recognition (Turk & Pentland, 1991) - Popularized Principal Component Analysis (PCA) for face recognition.
6.	Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection (Belhumeur, Hespanha, Kriegman, 1997) - Introduced Linear Discriminant Analysis (LDA) for improved face recognition.
7.	Object Recognition from Local Scale-Invariant Features (David Lowe, 2004, building on earlier work in 1999) - Introduced the Scale-Invariant Feature Transform (SIFT).
8.	Rapid Object Detection using a Boosted Cascade of Simple Features (Viola & Jones, 2001) - A highly influential real-time face detection framework.
9.	A Training Algorithm for Optimal Margin Classifiers (Boser, Guyon, Vapnik, 1992) - Foundational work on Support Vector Machines (SVMs).
10.	Induction of Decision Trees (J. Ross Quinlan, 1986) - Introduced the ID3 algorithm, a key work in decision tree learning.
11.	Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference (Judea Pearl, 1988) - Foundational work on Bayesian Networks.
12.	Nearest neighbor pattern classification (Cover & Hart, 1967) - Formalized the k-Nearest Neighbors algorithm.
The Deep Learning Revolution: Foundational Architectures & Breakthroughs
(Many of these are highlighted in Seed-1)
13.	Backpropagation Applied to Handwritten Zip Code Recognition (LeCun et al., 1989) - LeNet, showcasing early success of CNNs and backpropagation.
14.	A Fast Learning Algorithm for Deep Belief Nets (Hinton, Osindero, Teh, 2006) - Revived interest in deep architectures with unsupervised pre-training.
15.	ImageNet Classification with Deep Convolutional Neural Networks (Krizhevsky, Sutskever, Hinton, 2012) - AlexNet, the catalyst for the deep learning boom, leveraging GPUs.
16.	Very Deep Convolutional Networks for Large-Scale Image Recognition (Simonyan & Zisserman, 2014) - VGGNet, demonstrating the power of increased depth with small filters.
17.	Going Deeper with Convolutions (Szegedy et al., 2014) - GoogLeNet/Inception, introducing architectural innovations for efficiency and performance.
18.	Deep Residual Learning for Image Recognition (He, Zhang, Ren, Sun, 2015) - ResNet, enabling significantly deeper networks by using residual connections.
19.	EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (Tan & Le, 2019) - Introduced compound scaling for CNNs.
20.	An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020) - Vision Transformer (ViT), applying Transformers directly to image recognition.
Deep Learning in Natural Language Processing
(Many of these are highlighted in Seed-1)
21.	Efficient Estimation of Word Representations in Vector Space (Mikolov et al., 2013) - Introduced Word2Vec.
22.	GloVe: Global Vectors for Word Representation (Pennington, Socher, Manning, 2014) - Introduced GloVe.
23.	Sequence to Sequence Learning with Neural Networks (Sutskever, Vinyals, Le, 2014) - A foundational Seq2Seq paper for tasks like machine translation.
24.	Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau, Cho, Bengio, 2015) - Introduced the attention mechanism in NLP.
25.	Attention Is All You Need (Vaswani et al., 2017) - Introduced The Transformer architecture.
26.	Improving Language Understanding by Generative Pre-Training (Radford et al., 2018) - GPT-1.
27.	BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2018) - Introduced BERT.
28.	Language Models are Unsupervised Multitask Learners (Radford et al., 2019) - GPT-2.
29.	Language Models are Few-Shot Learners (Brown et al., 2020) - GPT-3.
Deep Learning in Face Recognition
30.	DeepFace: Closing the Gap to Human-Level Performance in Face Verification (Taigman et al., Facebook, 2014) - A landmark paper showcasing deep learning's power for face recognition.
31.	FaceNet: A Unified Embedding for Face Recognition and Clustering (Schroff, Kalenichenko, Philbin, Google, 2015) - Introduced the Triplet Loss and direct learning of embeddings for face verification.
32.	DeepID3: Face Recognition with Very Deep Neural Networks (Sun et al., 2015) - Part of the influential DeepID series, pushing accuracy with deeper networks and joint identification-verification signals.
33.	SphereFace: Deep Hypersphere Embedding for Face Recognition (Liu et al., 2017) - Introduced angular margin for more discriminative face embeddings.
34.	CosFace: Large Margin Cosine Loss for Deep Face Recognition (Wang et al., 2018) - Proposed a cosine margin penalty for improved learning.
35.	ArcFace: Additive Angular Margin Loss for Deep Face Recognition (Deng et al., 2019) - Introduced an additive angular margin, becoming a widely adopted loss function. 1
36.	An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020) - While general CV, this paper (ViT) and subsequent works like FaceXformer show Transformers being applied to face analysis.
Deep Learning in Reinforcement Learning
37.	Learning from Delayed Rewards (Watkins, 1989) - Ph.D. Thesis introducing Q-Learning.
38.	Playing Atari with Deep Reinforcement Learning (Mnih et al., DeepMind, 2013 (NIPS Workshop) / 2015 (Nature)) - Deep Q-Network (DQN), combining Q-learning with deep CNNs.
Generalization Techniques in Deep Learning
(Key papers and concepts as highlighted in Seed-2)
39.	Understanding the difficulty of training deep feedforward neural networks (Glorot & Bengio, 2010) - Introduced Xavier/Glorot Initialization.
40.	Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (He et al., 2015) - Introduced He Initialization (also the ResNet paper).
41.	Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (Ioffe & Szegedy, 2015) - Introduced Batch Normalization.
42.	Dropout: A Simple Way to Prevent Neural Networks from Overfitting (Srivastava, Hinton, et al., 2014) - Introduced Dropout.
43.	Adam: A Method for Stochastic Optimization (Kingma & Ba, 2014) - Introduced the Adam optimizer.
44.	Decoupled Weight Decay Regularization (Loshchilov & Hutter, 2017/2019) - Introduced AdamW.
45.	Rethinking the Inception Architecture for Computer Vision (Szegedy et al., 2016) - Discussed Label Smoothing.
46.	mixup: Beyond Empirical Risk Minimization (Zhang et al., 2017) - Introduced Mixup augmentation.
47.	CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features (Yun et al., 2019) - Introduced CutMix augmentation.
48.	Deep Networks with Stochastic Depth (Huang et al., 2016) - Introduced Stochastic Depth.
49.	AutoAugment: Learning Augmentation Strategies From Data (Cubuk et al., 2019) - Introduced AutoAugment.
50.	Sharpness-Aware Minimization for Efficiently Improving Generalization (Foret et al., 2020) - Introduced SAM optimizer.
________________________________________Recent Advances in Face Recognition (Post-2023)
51.	Transformer-Based Auxiliary Loss for Age-Invariant Face Recognition (Mahdi Abbasi, Sobhan Shafiei, Pooya Salehi, Akram Gholamzadeh, Mohammad H. Rohban, Mahdi Eftekhari, 2025) - Proposes adding a transformer-based auxiliary loss to existing metric learning losses (like ArcFace) to enhance age-invariant face recognition by capturing global and long-range dependencies crucial for handling aging effects. 2
○	Direct Link: https://arxiv.org/pdf/2412.02198
52.	Review of Demographic Fairness in Face Recognition (Ketan Kotwal, Sebastien Marcel, 2025) - A comprehensive review on demographic fairness in FR, examining causes of bias, available datasets, assessment metrics, and mitigation techniques, emphasizing the need for equitable systems. 4
○	Direct Link: https://arxiv.org/abs/2502.02309
53.	VariFace: Fair and Diverse Synthetic Dataset Generation for Face Recognition (Michael Yeung, Toya Teramoto, Songtao Wu, et al., 2024) - Introduces a two-stage diffusion-based pipeline to create synthetic face datasets that are demographically fair and diverse, reportedly outperforming models trained on some real-world datasets in unconstrained settings. 7
○	Direct Link: https://arxiv.org/abs/2412.06235
54.	FaceXFormer: A Unified Transformer for Facial Analysis (Kartik Narayan, Vibashan VS, Rama Chellappa, Vishal M. Patel, 2025) - An end-to-end unified transformer model for ten facial analysis tasks (including recognition, parsing, landmark detection, etc.) using a task-as-token approach and a lightweight decoder with bi-directional cross-attention. 11
○	Direct Link: https://kartik-3004.github.io/facexformer/ (Project Page), https://arxiv.org/abs/2403.12960 (arXiv)
55.	Face-LLaVA: Facial Expression and Attribute Understanding through Instruction Tuning (Ashutosh Chaubey, Xulang Guan, Mohammad Soleymani, 2025) - A multimodal large language model (MLLM) for face-centered in-context learning, handling tasks like expression/attribute recognition, AU detection, age estimation, and deepfake detection, and generating natural language reasoning for its predictions. 13
○	Direct Link: https://arxiv.org/abs/2504.07198
56.	A Comprehensive Review of Face Recognition Techniques, Trends and Challenges (Authors not explicitly listed in snippet, IEEE Access, 2024) - An in-depth review of FR methodologies, including feature extraction, pre-processing, face detection, classification, and notably, video-based FR, along with analysis of 2D and 3D FR datasets. 16
○	Direct Link: (Accessible via IEEE Xplore database)
Recent Breakthroughs in Natural Language Processing (Post-2023)
57.	LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models (Aida Kostikova, Zhipin Wang, Deidamea Bajri, Ole Pütz, Benjamin Paaßen, Steffen Eger, 2025) - A data-driven survey identifying trends in research on LLM limitations (2022-2024), noting reasoning, generalization, hallucination, bias, and security as key concerns, with arXiv papers showing a shift towards safety, controllability, and multimodality. 17
○	Direct Link: https://arxiv.org/abs/2505.19240
58.	Birdie: Efficient State Space Models through Bidirectional Input Processing and Specialized Pre-Training Objective Mixtures (Posu Chen, Hitu Seth, Parijat Dube, et al., 2024) - A novel training procedure for State Space Models (SSMs) to improve in-context retrieval capabilities, making SSMs more competitive with Transformers on recall-intensive tasks without architectural changes, using bidirectional input and RL-optimized objective mixtures. 20
○	Direct Link: https://arxiv.org/abs/2411.01030
59.	From N-grams to Transformers and Back: A Comparative Behavioral Framework for Bias in Language Models (Debjanee Barua, Gaoge WANG, Elissa M. Redmiles, Michelle Mazurek, Wojtek Palubicki, 2025) - Investigates bias propagation in n-gram vs. Transformer LMs, finding Transformers more robust to contextual bias and offering an interpretable framework to analyze bias propagation mechanisms. 22
○	Direct Link: https://arxiv.org/pdf/2505.12381
60.	Advancing Reasoning in Large Language Models: Promising Methods and Approaches (Avinash Patil, Aryan Jadon, 2025) - A survey reviewing emerging techniques to enhance LLM reasoning, categorized into prompting strategies (CoT, ToT), architectural innovations (retrieval-augmentation, neuro-symbolic), and learning paradigms (fine-tuning, RL). 23
○	Direct Link: https://arxiv.org/abs/2502.03671
61.	Empowering LLMs with Logical Reasoning: A Comprehensive Survey (Fengxiang Cheng, Haoxuan Li, Fenrong Liu, Robert van Rooij, Kun Zhang, Zhouchen Lin, 2025) - A survey focused on logical reasoning in LLMs, covering logical question answering and consistency, with a taxonomy of methods (solver-aided, prompting, pretraining/fine-tuning) and benchmarks. 25
○	Direct Link: https://arxiv.org/abs/2502.15652
62.	Cross-Images Contrastive Decoding: Precise, Lossless Suppression of Language Priors in Large Vision-Language Models (Jianfei Zhao, Feng Zhang, Xin Sun, Chong Feng, 2025) - Introduces CICD, a training-free method to reduce hallucinations in LVLMs by using different images for negative contexts, selectively suppressing detrimental language priors while preserving essential ones. 27
○	Direct Link: https://arxiv.org/abs/2505.10634
63.	Mitigating Hallucinations in Large Vision-Language Models via Summary-Guided Decoding (Kyungmin Min, Minbeom Kim, Kang-il Lee, Dongryeol Lee, Kyomin Jung, 2025) - Proposes SumGD, a decoding strategy to mitigate LVLM hallucinations by encouraging focus on image information through context summarization and selective control of image-related POS token generation. 30
○	Direct Link: https://aclanthology.org/2025.findings-naacl.235.pdf
64.	ConKE: Conceptualization-Augmented Knowledge Editing in Large Language Models for Commonsense Reasoning (Liyu Zhang, Weiqi Wang, Tianqing Fang, Yangqiu Song, 2025) - A knowledge editing framework for commonsense knowledge in LLMs, using an automated verifier to find errors and integrating conceptualization/instantiation for more generalizable edits. 31
○	Direct Link:(https://github.com/HKUST-KnowComp/ConKE) (Code), https://arxiv.org/abs/2412.11418 (Paper)
65.	Multimodal Large Language Models Can Significantly Advance Scientific Reasoning (Jiahua Dong, Zhibin Gou, Yifu Geng, et al., 2025) - A position paper arguing MLLMs can advance scientific reasoning by integrating diverse data types, reviewing current applications, challenges (open-source MLLMs lagging GPT-4o), and future steps. 34
○	Direct Link: https://arxiv.org/pdf/2502.02871
66.	Large Multimodal Models for Low-Resource Languages: A Survey (Mohamed Abdalla, Muhammad Abdul-Mageed, et al., 2025) - Surveys LMMs for low-resource languages (106 studies, 75 languages), highlighting the concentration on high-resource languages and discussing challenges/techniques for LR languages, where text-image pairings are most common. 35
○	Direct Link: https://arxiv.org/pdf/2502.05568
67.	Quantizing Large Language Models for Code Generation: A Differentiated Replication (Alessandro Giagnorio, Antonio Mastropaolo, Saima Afrin, Massimiliano Di Penta, Gabriele Bavota, 2025) - A replication study on quantizing LLMs for code generation (up to 34B parameters, down to 2-4 bits), finding 4-bit precision reduces memory by ~70% with limited performance impact; code-specific calibration helps for 2-3 bits. 36
○	Direct Link: https://doi.org/10.5281/zenodo.13752774 (Replication package), https://arxiv.org/abs/2503.07103 (Paper)
68.	When Reasoning Meets Compression: Benchmarking Compressed Large Reasoning Models on Complex Reasoning Tasks (Nan Zhang, Yusen Zhang, Prasenjit Mitra, Rui Zhang, 2025) - Benchmarks compressed DeepSeek-R1 (quantization, distillation, pruning) on complex reasoning tasks, finding parameter count impacts knowledge memorization more than reasoning, and shorter outputs generally perform better. 38
○	Direct Link: https://arxiv.org/abs/2504.02010
Recent Innovations in Computer Vision (Post-2023)
69.	ECViT: Efficient Convolutional Vision Transformer with Local-Attention and Multi-scale Stages (Zhoujie Qian, 2025) - A hybrid CNN-Transformer architecture (ECViT) that introduces CNN inductive biases (locality, translation invariance) and uses local attention and a pyramid structure for efficient multi-scale feature extraction, balancing performance and computational cost. 40
○	Direct Link: https://arxiv.org/abs/2504.14825
70.	UniViTAR: Unified Vision Transformer with Native Resolution (Limeng Qiao, et al., 2025) - A ViT-based foundation model family (UniViTAR) designed to process images/videos at native resolutions and dynamic aspect ratios, incorporating LLM-inspired upgrades (2D RoPE, SwiGLU, RMSNorm) and progressive training. 43
○	Direct Link: https://arxiv.org/abs/2504.01792
71.	Generative Physical AI in Vision: A Survey (Daochang Liu, Junyu Zhang, Anh-Dung Dinh, Eunbyung Park, Shichao Zhang, Ajmal Mian, Mubarak Shah, Chang Xu, 2025) - A survey on physics-aware generative models in CV, which aim for physically plausible content by incorporating physical knowledge explicitly (simulation) or implicitly (learning from data), towards "world simulators." 46
○	Direct Link: https://arxiv.org/abs/2501.10928 (Summary: https://tinyurl.com/Physics-Aware-Generation)
72.	VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness (Large collaborative effort, 2025) - A benchmark suite for advanced video generation models (e.g., Sora, Kling, Veo 2) focusing on "intrinsic faithfulness" across five dimensions: Human Fidelity, Controllability, Creativity, Physics, and Commonsense. 47
○	Direct Link: https://arxiv.org/abs/2503.21755
73.	Controllable 3D Outdoor Scene Generation via Scene Graphs (Yiran Xing, Zhaoxiang Cai, et al., 2025) - Proposes generating controllable outdoor 3D scenes using scene graphs as an intuitive control format, with an interactive system transforming sparse scene graphs to dense BEV Embedding Maps to guide a conditional diffusion model. 10
○	Direct Link: https://arxiv.org/abs/2503.07152
74.	Escaping the Big Data Paradigm in Self-Supervised Representation Learning from Images (Carlos Velez-García, Miguel Cazorla, Jorge Pomares, 2025) - Introduces SCOTT (Sparse Convolutional Tokenizer for Transformers) and MIM-JEPA (Masked Image Modeling with Joint-Embedding Predictive Architecture) to train ViTs with SSL on much smaller datasets without external pretraining, aiming for more accessible SSL. 53
○	Direct Link: https://github.com/inescopresearch/scott (Code), https://arxiv.org/abs/2502.18056 (Paper)
75.	UpStep: Unsupervised Parameter-efficient Source-free Post-pretraining (Ulas Gul, Oguz Kaan Yüksel, et al., 2025) - An unsupervised, parameter-efficient, source-free post-pretraining method (UpStep) to adapt pretrained visual models to unlabeled target domains using SSL and "center vector regularization" (CVR) to minimize catastrophic forgetting and reduce computational costs. 55
○	Direct Link: https://www.arxiv.org/pdf/2502.21313 (Paper PDF)
Recent Developments in Reinforcement Learning (Post-2023)
76.	A Survey on Explainable Deep Reinforcement Learning (Zelei Cheng, Jiahao Yu, Xinyu Xing, 2025) - A survey on XRL, categorizing explanation techniques (feature, state, dataset, model-level), evaluation frameworks, and exploring XRL's role in policy refinement, robustness, security, and its integration with LLMs/RLHF. 56
○	Direct Link: https://arxiv.org/abs/2502.06869
77.	Video-Enhanced Offline RL (VeoRL): A Model-Based Approach (Yecheng Moon, et al., 2025) - A model-based offline RL method (VeoRL) that constructs an interactive world model enhanced with diverse, unlabeled online video data, using a hierarchical world model to predict state evolution and environmental feedback. 14
○	Direct Link: https://arxiv.org/abs/2505.06482
78.	Variational OOD State Correction for Offline Reinforcement Learning (Zicheng Zhang, et al., 2025) - Introduces Density-Aware Safety Perception (DASP) for OOD state correction in offline RL, encouraging agents to prioritize actions leading to higher data density outcomes within a variational framework. 14
○	Direct Link: https://arxiv.org/abs/2505.00503
79.	Addressing Rotational Learning Dynamics in Multi-Agent Reinforcement Learning (Di-An Jan, et al., 2025) - Tackles instability in MARL from "rotational optimization dynamics" by reframing MARL using Variational Inequalities (VIs) and proposing LA-MARL and EG-MARL to integrate gradient-based VI methods into MARL algorithms. 2
○	Direct Link: https://arxiv.org/abs/2410.07976
80.	Offline Multi-agent Reinforcement Learning via Score Decomposition (Lingheng Meng, et al., 2025) - Addresses offline MARL challenges (distributional shifts, high-dimensional action spaces, coordination diversity) with a two-stage framework using a diffusion model to capture joint behavior policy and sequential score function decomposition for regularization. 2
○	Direct Link: https://arxiv.org/abs/2505.05968
81.	A Shared Low-Rank Adaptation Approach to Personalized RLHF (Renpu Liu, Peng Wang, Donghao Li, Cong Shen, Jing Yang, 2025) - Introduces P-ShareLoRA for personalized RLHF, leveraging shared LoRA components to efficiently learn personalized reward functions, addressing homogeneous human preference assumptions in standard RLHF. 4
○	Direct Link: https://arxiv.org/abs/2503.19201
82.	MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions (Yekun Chai, Haoran Sun, Huang Fang, Shuohuan Wang, Yu Sun, Hua Wu, 2025) - Proposes MA-RLHF to improve token-level RLHF efficiency for long text generation by incorporating "macro-actions" (token sequences or higher-level constructs) to address credit assignment problems. 4
○	Direct Link:(https://github.com/ernie-research/MA-RLHF) (Code), https://arxiv.org/abs/2410.02743 (Paper)
83.	CORL: Clean Offline Reinforcement Learning (Authors of CORL library, presented at NeurIPS 2023) - An open-source library providing benchmarked single-file implementations of deep offline and offline-to-online RL algorithms, emphasizing simplicity and experiment tracking. 70
○	Direct Link: (NeurIPS 2023 proceedings/CORL GitHub)
Recent Progress in General ML Theory and Optimization (Post-2023)
84.	ZeroFlow: Overcoming Catastrophic Forgetting is Easier than You Think (Tao Feng, Wei Li, Didi Zhu, Hangjie Yuan, Wendi Zheng, Dan Zhang, Jie Tang, 2025) - Introduces ZeroFlow, the first benchmark for evaluating gradient-free (zeroth-order) optimization algorithms for continual learning, finding forward passes alone can mitigate catastrophic forgetting and revealing new optimization principles. 71
○	Direct Link: https://arxiv.org/abs/2501.01045
85.	LORENZA: Enhancing Generalization in Low-Rank Gradient LLM Training and Fine-Tuning via Efficient Zeroth-Order Adaptive SAM Optimization (Yehonathan Refael, Iftach Arbel, Ofir Lindenbaum, Tom Tirer, 2025) - Proposes AdaZo-SAM (Adam + SAM with single gradient computation via zeroth-order estimation) and LORENZA (memory-efficient AdaZo-SAM with adaptive low-rank gradient updates) to improve generalization for LLM PEFT and pre-training. 15
○	Direct Link: https://arxiv.org/abs/2502.19571
86.	Deep Multi-Task Learning Has Low Amortized Intrinsic Dimensionality (Hossein Zakerinia, et al., 2025) - Confirms deep MTL models learn within a low intrinsic dimensional subspace and introduces a method to parameterize them directly in this space, leading to the first non-vacuous generalization bounds for deep MTL using weight compression and PAC-Bayesian reasoning. 10
○	Direct Link: https://arxiv.org/abs/2501.19067
87.	Survey on Generalization Theory for Graph Neural Networks (Kajetan Schweighofer, et al., 2025) - Systematically reviews literature on the generalization abilities of Message-Passing Neural Networks (MPNNs), analyzing strengths/limitations of theoretical studies using VC dimension, Rademacher complexity, stability, PAC-Bayesian bounds, etc. 10
○	Direct Link: https://arxiv.org/pdf/2503.15650
88.	Ferret: Federated Full-Parameter Tuning at Scale for Large Language Models (Yao Shu, Wenyang Hu, See-Kiong Ng, Bryan Kian Hsiang Low, Fei Richard Yu, 2024) - Proposes Ferret, the first first-order FL method using shared randomness for scalable full-parameter LLM tuning, balancing accuracy, efficiency, communication overhead, and convergence. 10
○	Direct Link: https://github.com/allen4747/Ferret (Code), https://arxiv.org/abs/2409.06277 (Paper)
89.	FedDDL: Federated Deconfounding and Debiasing Learning for Out-of-Distribution Generalization (Qi Zhuang, Ming-Chang Lee, Han Yu, et al., 2025) - Addresses attribute bias in FL causing poor OOD generalization, proposing FedDDL with a Disentangled Effect Calibration (DEC) module to decouple background/object features and generate counterfactuals for robust causal feature learning. 84
○	Direct Link: https://arxiv.org/abs/2505.04979 (Paper)
90.	FLTG: Byzantine-Robust Federated Learning via Angle-Based Defense and Non-IID-Aware Weighting (Jiahao Wang, et al., 2025) - A Byzantine-robust FL aggregation algorithm (FLTG) using angle-based defense (cosine similarity with server's clean dataset) to filter malicious updates, with dynamic reference selection and non-IID-aware weighting. 86
○	Direct Link: https://arxiv.org/abs/2505.12851
91.	Communication-efficient Vertical Federated Learning via Compressed Error Feedback (Pedro Valdeira, João Xavier, Cláudia Soares, Yuejie Chi, 2024) - Introduces EF-VFL, an error feedback compressed vertical FL method for split neural networks, using lossy compression and error feedback to maintain convergence, supporting private labels. 87
○	Direct Link: https://github.com/pedromvaldeira/EF-VFL (Code), https://arxiv.org/abs/2406.14420 (Paper)
92.	A Statistical Case Against Empirical Human-AI Alignment (Julian Rodemann, Esteban Garces Arias, Christoph Luther, Christoph Jansen, Thomas Augustin, 2025) - Critiques naive empirical human-AI alignment (especially forward/a priori), arguing it can introduce statistical biases, and advocates for prescriptive alignment and a posteriori empirical alignment. 4
○	Direct Link: https://arxiv.org/abs/2502.14581
This list provides a snapshot of the pivotal works and concepts that have shaped the field, reflecting the historical progression and recent research breakthroughs discussed.
Works cited
1.	ArcFace - Additive Angular Margin Loss For Deep Face Recognition - Scribd, accessed June 1, 2025, https://www.scribd.com/document/706535195/ArcFace-Additive-Angular-Margin-Loss-for-Deep-Face-Recognition
2.	Transformer-Based Auxiliary Loss for Face Recognition Across Age Variations - arXiv, accessed June 1, 2025, https://arxiv.org/pdf/2412.02198?
3.	arxiv.org, accessed June 1, 2025, https://arxiv.org/abs/2412.02198
4.	Review of Demographic Fairness in Face Recognition - arXiv, accessed June 1, 2025, https://arxiv.org/html/2502.02309v2
5.	arxiv.org, accessed June 1, 2025, https://arxiv.org/abs/2502.02309
6.	Review of Demographic Fairness in Face Recognition - arXiv, accessed June 1, 2025, https://arxiv.org/pdf/2502.02309
7.	VariFace: Fair and Diverse Synthetic Dataset Generation for Face Recognition - arXiv, accessed June 1, 2025, https://arxiv.org/html/2412.06235v2
8.	[Literature Review] VariFace: Fair and Diverse Synthetic Dataset Generation for Face Recognition - Moonlight | AI Colleague for Research Papers, accessed June 1, 2025, https://www.themoonlight.io/en/review/variface-fair-and-diverse-synthetic-dataset-generation-for-face-recognition
9.	VariFace: Fair and Diverse Synthetic Dataset Generation for Face Recognition, accessed June 1, 2025, https://www.researchgate.net/publication/386577486_VariFace_Fair_and_Diverse_Synthetic_Dataset_Generation_for_Face_Recognition
10.	arxiv.org, accessed June 1, 2025, https://arxiv.org/pdf/2412.06235.pdf
11.	arxiv.org, accessed June 1, 2025, https://arxiv.org/abs/2403.12960
12.	FaceXFormer: A Unified Transformer for Facial Analysis - arXiv, accessed June 1, 2025, https://arxiv.org/html/2403.12960v3
13.	Face-LLaVA: Facial Expression and Attribute Understanding through Instruction Tuning, accessed June 1, 2025, https://arxiv.org/html/2504.07198
14.	[2504.07198] Face-LLaVA: Facial Expression and Attribute Understanding through Instruction Tuning - arXiv, accessed June 1, 2025, https://arxiv.org/abs/2504.07198
15.	[Literature Review] Face-LLaVA: Facial Expression and Attribute Understanding through Instruction Tuning - Moonlight, accessed June 1, 2025, https://www.themoonlight.io/en/review/face-llava-facial-expression-and-attribute-understanding-through-instruction-tuning
16.	A Comprehensive Review of Face Recognition Techniques, Trends and Challenges, accessed June 1, 2025, https://www.researchgate.net/publication/382098748_A_Comprehensive_Review_of_Face_Recognition_Techniques_Trends_and_Challenges
17.	LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models - arXiv, accessed June 1, 2025, https://arxiv.org/html/2505.19240v1
18.	[2505.19240] LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models - arXiv, accessed June 1, 2025, https://arxiv.org/abs/2505.19240
19.	Artificial Intelligence May 2025 - arXiv, accessed June 1, 2025, http://arxiv.org/list/cs.AI/2025-05?skip=3400&show=25
20.	arXiv:2411.01030v5 [cs.CL] 21 Feb 2025, accessed June 1, 2025, https://arxiv.org/pdf/2411.01030?
21.	Understanding the Skill Gap in Recurrent Language Models: The Role of the Gather-and-Aggregate Mechanism - arXiv, accessed June 1, 2025, https://arxiv.org/pdf/2504.18574
22.	arXiv:2505.12381v1 [cs.CL] 18 May 2025, accessed June 1, 2025, https://arxiv.org/pdf/2505.12381
23.	Advancing Reasoning in Large Language Models: Promising Methods and Approaches, accessed June 1, 2025, https://arxiv.org/html/2502.03671v2
24.	arxiv.org, accessed June 1, 2025, https://arxiv.org/abs/2502.03671
25.	arXiv:2502.15652v2 [cs.AI] 24 Feb 2025, accessed June 1, 2025, https://arxiv.org/pdf/2502.15652?
26.	arxiv.org, accessed June 1, 2025, https://arxiv.org/abs/2502.15652
27.	Mitigate Language Priors in Large Vision-Language Models by Cross-Images Contrastive Decoding - arXiv, accessed June 1, 2025, https://arxiv.org/html/2505.10634v2
28.	[2505.10634] Cross-Image Contrastive Decoding: Precise, Lossless Suppression of Language Priors in Large Vision-Language Models - arXiv, accessed June 1, 2025, https://arxiv.org/abs/2505.10634
29.	Mitigate Language Priors in Large Vision-Language Models by Cross-Images Contrastive Decoding - arXiv, accessed June 1, 2025, https://arxiv.org/html/2505.10634v1
30.	aclanthology.org, accessed June 1, 2025, https://aclanthology.org/2025.findings-naacl.235.pdf
31.	ConKE: Conceptualization-Augmented Knowledge Editing in Large Language Models for Commonsense Reasoning - arXiv, accessed June 1, 2025, https://arxiv.org/html/2412.11418v2
32.	arXiv:2412.11418v2 [cs.CL] 28 May 2025, accessed June 1, 2025, http://arxiv.org/pdf/2412.11418
33.	arXiv:2412.11418v2 [cs.CL] 28 May 2025, accessed June 1, 2025, https://arxiv.org/abs/2412.11418
34.	Position: Multimodal Large Language Models Can Significantly Advance Scientific Reasoning - arXiv, accessed June 1, 2025, https://arxiv.org/pdf/2502.02871?
35.	arXiv:2502.05568v1 [cs.CL] 8 Feb 2025, accessed June 1, 2025, https://arxiv.org/pdf/2502.05568?
36.	Quantizing Large Language Models for Code Generation: A Differentiated Replication, accessed June 1, 2025, https://arxiv.org/html/2503.07103v1
37.	arxiv.org, accessed June 1, 2025, https://arxiv.org/abs/2503.07103
38.	arXiv:2504.02010v1 [cs.LG] 2 Apr 2025, accessed June 1, 2025, https://arxiv.org/pdf/2504.02010
39.	arxiv.org, accessed June 1, 2025, https://arxiv.org/abs/2504.02010
40.	ECViT: Efficient Convolutional Vision Transformer with Local-Attention and Multi-scale Stages - arXiv, accessed June 1, 2025, https://arxiv.org/html/2504.14825v1
41.	[2504.14825] ECViT: Efficient Convolutional Vision Transformer with Local-Attention and Multi-scale Stages - arXiv, accessed June 1, 2025, https://arxiv.org/abs/2504.14825
42.	Artificial Intelligence Apr 2025 - arXiv, accessed June 1, 2025, http://arxiv.org/list/cs.AI/2025-04?skip=1275&show=1000
43.	UniViTAR: Unified Vision Transformer with Native Resolution - arXiv, accessed June 1, 2025, https://arxiv.org/html/2504.01792v1
44.	UniViTAR: Unified Vision Transformer with Native Resolution - arXiv, accessed June 1, 2025, https://arxiv.org/html/2504.01792v2
45.	[2504.01792] UniViTAR: Unified Vision Transformer with Native Resolution - arXiv, accessed June 1, 2025, https://arxiv.org/abs/2504.01792
46.	Generative Physical AI in Vision: A Survey - arXiv, accessed June 1, 2025, https://arxiv.org/html/2501.10928v2
47.	arxiv.org, accessed June 1, 2025, https://arxiv.org/abs/2501.10928
48.	VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness - arXiv, accessed June 1, 2025, https://arxiv.org/html/2503.21755v1
49.	arxiv.org, accessed June 1, 2025, https://arxiv.org/abs/2503.21755
50.	SkyReels-V2: Infinite-length Film Generative Model - ResearchGate, accessed June 1, 2025, https://www.researchgate.net/publication/390893117_SkyReels-V2_Infinite-length_Film_Generative_Model
51.	Controllable 3D Outdoor Scene Generation via Scene Graphs - arXiv, accessed June 1, 2025, https://arxiv.org/html/2503.07152v1
52.	Controllable 3D Outdoor Scene Generation via Scene Graphs | Request PDF - ResearchGate, accessed June 1, 2025, https://www.researchgate.net/publication/389715074_Controllable_3D_Outdoor_Scene_Generation_via_Scene_Graphs
53.	arXiv:2502.18056v1 [cs.CV] 25 Feb 2025, accessed June 1, 2025, https://arxiv.org/pdf/2502.18056
54.	arxiv.org, accessed June 1, 2025, https://arxiv.org/abs/2502.18056
55.	arXiv:2502.21313v1 [cs.CV] 28 Feb 2025, accessed June 1, 2025, https://www.arxiv.org/pdf/2502.21313
56.	A Survey on Explainable Deep Reinforcement Learning - arXiv, accessed June 1, 2025, https://arxiv.org/html/2502.06869v1
57.	arxiv.org, accessed June 1, 2025, https://arxiv.org/abs/2502.06869
58.	arXiv:2502.06869v1 [cs.LG] 8 Feb 2025, accessed June 1, 2025, https://arxiv.org/pdf/2502.06869?
59.	Video-Enhanced Offline Reinforcement Learning: A Model-Based Approach - arXiv, accessed June 1, 2025, https://arxiv.org/html/2505.06482v2
60.	Variational OOD State Correction for Offline Reinforcement Learning - arXiv, accessed June 1, 2025, https://arxiv.org/html/2505.00503v1
61.	Addressing Rotational Learning Dynamics in Multi-Agent Reinforcement Learning - arXiv, accessed June 1, 2025, https://arxiv.org/html/2410.07976v2
62.	arxiv.org, accessed June 1, 2025, https://arxiv.org/abs/2410.07976
63.	Offline Multi-agent Reinforcement Learning via Score Decomposition - arXiv, accessed June 1, 2025, https://arxiv.org/html/2505.05968v1
64.	[2505.05968] Offline Multi-agent Reinforcement Learning via Score Decomposition - arXiv, accessed June 1, 2025, https://arxiv.org/abs/2505.05968
65.	Machine Learning May 2025 - arXiv, accessed June 1, 2025, https://www.arxiv.org/list/cs.LG/2025-05?skip=275&show=250
66.	arXiv:2503.19201v1 [cs.LG] 24 Mar 2025, accessed June 1, 2025, https://arxiv.org/pdf/2503.19201
67.	arxiv.org, accessed June 1, 2025, https://arxiv.org/abs/2503.19201
68.	ma-rlhf: reinforcement learning from hu - arXiv, accessed June 1, 2025, https://arxiv.org/pdf/2410.02743?
69.	arxiv.org, accessed June 1, 2025, https://arxiv.org/abs/2410.02743
70.	Research-oriented Deep Offline Reinforcement Learning Library - NeurIPS Poster CORL, accessed June 1, 2025, https://neurips.cc/virtual/2023/poster/73613
71.	ZeroFlow: Overcoming Catastrophic Forgetting is Easier than You Think - arXiv, accessed June 1, 2025, https://arxiv.org/html/2501.01045v3
72.	[2501.01045] ZeroFlow: Overcoming Catastrophic Forgetting is Easier than You Think, accessed June 1, 2025, https://arxiv.org/abs/2501.01045
73.	arxiv.org, accessed June 1, 2025, https://arxiv.org/abs/2505.04979
74.	Computer Science Jan 2025 - arXiv, accessed June 1, 2025, http://arxiv.org/list/cs/2025-01?skip=250&show=1000
75.	LORENZA: Enhancing Generalization in Low-Rank Gradient LLM Training and Fine-Tuning via Efficient Zeroth-Order Adaptive SAM Optimization - arXiv, accessed June 1, 2025, https://arxiv.org/html/2502.19571v1
76.	arxiv.org, accessed June 1, 2025, https://arxiv.org/abs/2502.19571
77.	Deep Multi-Task Learning Has Low Amortized Intrinsic Dimensionality - arXiv, accessed June 1, 2025, https://arxiv.org/html/2501.19067v1
78.	[2501.19067] Deep Multi-Task Learning Has Low Amortized Intrinsic Dimensionality - arXiv, accessed June 1, 2025, https://arxiv.org/abs/2501.19067
79.	Survey on Generalization Theory for Graph Neural Networks arXiv:2503.15650v1 [cs.LG] 19 Mar 2025, accessed June 1, 2025, https://arxiv.org/pdf/2503.15650
80.	Publications | GLOW.AI - NUS Computing, accessed June 1, 2025, https://www.comp.nus.edu.sg/~lowkh/publications.html
81.	Ferret: Federated Full-Parameter Tuning at Scale for Large Language Models - arXiv, accessed June 1, 2025, https://arxiv.org/pdf/2409.06277
82.	Ferret: Federated Full-Parameter Tuning at Scale for Large Language Models - arXiv, accessed June 1, 2025, https://arxiv.org/html/2409.06277v2
83.	arxiv.org, accessed June 1, 2025, https://arxiv.org/abs/2409.06277
84.	Han Yu's Homepage - The Federated Learning Portal, accessed June 1, 2025, https://federated-learning.org/han.yu/
85.	Federated Deconfounding and Debiasing Learning for Out-of-Distribution Generalization - arXiv, accessed June 1, 2025, https://arxiv.org/html/2505.04979v2
86.	FLTG: Byzantine-Robust Federated Learning via Angle-Based Defense and Non-IID-Aware Weighting - arXiv, accessed June 1, 2025, https://arxiv.org/html/2505.12851v1
87.	[2406.14420] Communication-efficient Vertical Federated Learning via Compressed Error Feedback - arXiv, accessed June 1, 2025, https://arxiv.org/abs/2406.14420
88.	A Statistical Case Against Empirical Human–AI Alignment - arXiv, accessed June 1, 2025, https://arxiv.org/html/2502.14581v1
89.	A Statistical Case Against Empirical Human–AI Alignment - arXiv, accessed June 1, 2025, https://arxiv.org/pdf/2502.14581
90.	arxiv.org, accessed June 1, 2025, https://arxiv.org/abs/2502.14581
