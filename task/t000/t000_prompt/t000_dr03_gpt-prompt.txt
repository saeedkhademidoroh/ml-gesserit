> Gemini Prompt

You are now continuing a deep research workflow based on our expanding seed corpus.

Please use the newly provided article list (`t000_dr03_article-list`) and associated source document (`t000_dr03_article-source`) as your primary input.

Your task:
1. Crawl and analyze the articles listed.
2. Extract *new relevant papers* referenced by these articles or related works cited alongside them.
3. Focus on identifying:
   - State-of-the-art (SOTA) contributions
   - Highly cited or influential works
   - Recent breakthroughs (2024–2025)
   - Emerging methods, architectures, or paradigms
4. Include metadata such as title, authors, publication year, and brief abstract or contribution statement.

Goal:
Build a broader research map by recursively expanding this seed set. Prioritize references that introduce impactful innovations, inspire follow-ups, or shift paradigms.

Do **not** repeat articles already in `t000_dr03_article-list`.

The new expanded list should be saved as: `t000_dr04_article-list`.

(References to all files mentioned will be provided in the description.)

> Gemini Seed

Based on the comprehensive historical analysis conducted, here is a categorized list of influential articles, foundational works, and key concepts that represent significant milestones in the evolution of machine learning, with an emphasis on face recognition and deep learning, now updated with recent research:
Early Foundational Milestones & Concepts
1.    Computing Machinery and Intelligence (Alan Turing, 1950) - Introduced the "Turing Test" as a measure of machine intelligence.
2.    The Perceptron: A Probabilistic Model for Information Storage and Organization in the Brain (Frank Rosenblatt, 1958) - Introduced the Perceptron algorithm.
3.    A Logical Calculus of the Ideas Immanent in Nervous Activity (McCulloch & Pitts, 1943) - Proposed early mathematical models of neurons.
4.    Neocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position (Kunihiko Fukushima, 1980) - A precursor to Convolutional Neural Networks.
Pre-Deep Learning Era: Key Algorithms & Techniques
5.    Eigenfaces for Recognition (Turk & Pentland, 1991) - Popularized Principal Component Analysis (PCA) for face recognition.
6.    Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection (Belhumeur, Hespanha, Kriegman, 1997) - Introduced Linear Discriminant Analysis (LDA) for improved face recognition.
7.    Object Recognition from Local Scale-Invariant Features (David Lowe, 2004, building on earlier work in 1999) - Introduced the Scale-Invariant Feature Transform (SIFT).
8.    Rapid Object Detection using a Boosted Cascade of Simple Features (Viola & Jones, 2001) - A highly influential real-time face detection framework.
9.    A Training Algorithm for Optimal Margin Classifiers (Boser, Guyon, Vapnik, 1992) - Foundational work on Support Vector Machines (SVMs).
10.    Induction of Decision Trees (J. Ross Quinlan, 1986) - Introduced the ID3 algorithm, a key work in decision tree learning.
11.    Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference (Judea Pearl, 1988) - Foundational work on Bayesian Networks.
12.    Nearest neighbor pattern classification (Cover & Hart, 1967) - Formalized the k-Nearest Neighbors algorithm.
The Deep Learning Revolution: Foundational Architectures & Breakthroughs
(Many of these are highlighted in Seed-1)
13.    Backpropagation Applied to Handwritten Zip Code Recognition (LeCun et al., 1989) - LeNet, showcasing early success of CNNs and backpropagation.
14.    A Fast Learning Algorithm for Deep Belief Nets (Hinton, Osindero, Teh, 2006) - Revived interest in deep architectures with unsupervised pre-training.
15.    ImageNet Classification with Deep Convolutional Neural Networks (Krizhevsky, Sutskever, Hinton, 2012) - AlexNet, the catalyst for the deep learning boom, leveraging GPUs.
16.    Very Deep Convolutional Networks for Large-Scale Image Recognition (Simonyan & Zisserman, 2014) - VGGNet, demonstrating the power of increased depth with small filters.
17.    Going Deeper with Convolutions (Szegedy et al., 2014) - GoogLeNet/Inception, introducing architectural innovations for efficiency and performance.
18.    Deep Residual Learning for Image Recognition (He, Zhang, Ren, Sun, 2015) - ResNet, enabling significantly deeper networks by using residual connections.
19.    EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks (Tan & Le, 2019) - Introduced compound scaling for CNNs.
20.    An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020) - Vision Transformer (ViT), applying Transformers directly to image recognition.
Deep Learning in Natural Language Processing
(Many of these are highlighted in Seed-1)
21.    Efficient Estimation of Word Representations in Vector Space (Mikolov et al., 2013) - Introduced Word2Vec.
22.    GloVe: Global Vectors for Word Representation (Pennington, Socher, Manning, 2014) - Introduced GloVe.
23.    Sequence to Sequence Learning with Neural Networks (Sutskever, Vinyals, Le, 2014) - A foundational Seq2Seq paper for tasks like machine translation.
24.    Neural Machine Translation by Jointly Learning to Align and Translate (Bahdanau, Cho, Bengio, 2015) - Introduced the attention mechanism in NLP.
25.    Attention Is All You Need (Vaswani et al., 2017) - Introduced The Transformer architecture.
26.    Improving Language Understanding by Generative Pre-Training (Radford et al., 2018) - GPT-1.
27.    BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al., 2018) - Introduced BERT.
28.    Language Models are Unsupervised Multitask Learners (Radford et al., 2019) - GPT-2.
29.    Language Models are Few-Shot Learners (Brown et al., 2020) - GPT-3.
Deep Learning in Face Recognition
30.    DeepFace: Closing the Gap to Human-Level Performance in Face Verification (Taigman et al., Facebook, 2014) - A landmark paper showcasing deep learning's power for face recognition.
31.    FaceNet: A Unified Embedding for Face Recognition and Clustering (Schroff, Kalenichenko, Philbin, Google, 2015) - Introduced the Triplet Loss and direct learning of embeddings for face verification.
32.    DeepID3: Face Recognition with Very Deep Neural Networks (Sun et al., 2015) - Part of the influential DeepID series, pushing accuracy with deeper networks and joint identification-verification signals.
33.    SphereFace: Deep Hypersphere Embedding for Face Recognition (Liu et al., 2017) - Introduced angular margin for more discriminative face embeddings.
34.    CosFace: Large Margin Cosine Loss for Deep Face Recognition (Wang et al., 2018) - Proposed a cosine margin penalty for improved learning.
35.    ArcFace: Additive Angular Margin Loss for Deep Face Recognition (Deng et al., 2019) - Introduced an additive angular margin, becoming a widely adopted loss function. 1
36.    An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (Dosovitskiy et al., 2020) - While general CV, this paper (ViT) and subsequent works like FaceXformer show Transformers being applied to face analysis.
Deep Learning in Reinforcement Learning
37.    Learning from Delayed Rewards (Watkins, 1989) - Ph.D. Thesis introducing Q-Learning.
38.    Playing Atari with Deep Reinforcement Learning (Mnih et al., DeepMind, 2013 (NIPS Workshop) / 2015 (Nature)) - Deep Q-Network (DQN), combining Q-learning with deep CNNs.
Generalization Techniques in Deep Learning
(Key papers and concepts as highlighted in Seed-2)
39.    Understanding the difficulty of training deep feedforward neural networks (Glorot & Bengio, 2010) - Introduced Xavier/Glorot Initialization.
40.    Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification (He et al., 2015) - Introduced He Initialization (also the ResNet paper).
41.    Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (Ioffe & Szegedy, 2015) - Introduced Batch Normalization.
42.    Dropout: A Simple Way to Prevent Neural Networks from Overfitting (Srivastava, Hinton, et al., 2014) - Introduced Dropout.
43.    Adam: A Method for Stochastic Optimization (Kingma & Ba, 2014) - Introduced the Adam optimizer.
44.    Decoupled Weight Decay Regularization (Loshchilov & Hutter, 2017/2019) - Introduced AdamW.
45.    Rethinking the Inception Architecture for Computer Vision (Szegedy et al., 2016) - Discussed Label Smoothing.
46.    mixup: Beyond Empirical Risk Minimization (Zhang et al., 2017) - Introduced Mixup augmentation.
47.    CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features (Yun et al., 2019) - Introduced CutMix augmentation.
48.    Deep Networks with Stochastic Depth (Huang et al., 2016) - Introduced Stochastic Depth.
49.    AutoAugment: Learning Augmentation Strategies From Data (Cubuk et al., 2019) - Introduced AutoAugment.
50.    Sharpness-Aware Minimization for Efficiently Improving Generalization (Foret et al., 2020) - Introduced SAM optimizer.
Recent Advances in Face Recognition (Post-2023)
51.    Transformer-Based Auxiliary Loss for Age-Invariant Face Recognition (Mahdi Abbasi, Sobhan Shafiei, Pooya Salehi, Akram Gholamzadeh, Mohammad H. Rohban, Mahdi Eftekhari, 2025) - Proposes adding a transformer-based auxiliary loss to existing metric learning losses (like ArcFace) to enhance age-invariant face recognition by capturing global and long-range dependencies crucial for handling aging effects. 2
○    Direct Link: https://arxiv.org/pdf/2412.02198
52.    Review of Demographic Fairness in Face Recognition (Ketan Kotwal, Sebastien Marcel, 2025) - A comprehensive review on demographic fairness in FR, examining causes of bias, available datasets, assessment metrics, and mitigation techniques, emphasizing the need for equitable systems. 4
○    Direct Link: https://arxiv.org/abs/2502.02309
53.    VariFace: Fair and Diverse Synthetic Dataset Generation for Face Recognition (Michael Yeung, Toya Teramoto, Songtao Wu, et al., 2024) - Introduces a two-stage diffusion-based pipeline to create synthetic face datasets that are demographically fair and diverse, reportedly outperforming models trained on some real-world datasets in unconstrained settings. 7
○    Direct Link: https://arxiv.org/abs/2412.06235
54.    FaceXFormer: A Unified Transformer for Facial Analysis (Kartik Narayan, Vibashan VS, Rama Chellappa, Vishal M. Patel, 2025) - An end-to-end unified transformer model for ten facial analysis tasks (including recognition, parsing, landmark detection, etc.) using a task-as-token approach and a lightweight decoder with bi-directional cross-attention. 11
○    Direct Link: https://kartik-3004.github.io/facexformer/ (Project Page), https://arxiv.org/abs/2403.12960 (arXiv)
55.    Face-LLaVA: Facial Expression and Attribute Understanding through Instruction Tuning (Ashutosh Chaubey, Xulang Guan, Mohammad Soleymani, 2025) - A multimodal large language model (MLLM) for face-centered in-context learning, handling tasks like expression/attribute recognition, AU detection, age estimation, and deepfake detection, and generating natural language reasoning for its predictions. 13
○    Direct Link: https://arxiv.org/abs/2504.07198
56.    A Comprehensive Review of Face Recognition Techniques, Trends and Challenges (Authors not explicitly listed in snippet, IEEE Access, 2024) - An in-depth review of FR methodologies, including feature extraction, pre-processing, face detection, classification, and notably, video-based FR, along with analysis of 2D and 3D FR datasets. 16
○    Direct Link: (Accessible via IEEE Xplore database)
Recent Breakthroughs in Natural Language Processing (Post-2023)
57.    LLLMs: A Data-Driven Survey of Evolving Research on Limitations of Large Language Models (Aida Kostikova, Zhipin Wang, Deidamea Bajri, Ole Pütz, Benjamin Paaßen, Steffen Eger, 2025) - A data-driven survey identifying trends in research on LLM limitations (2022-2024), noting reasoning, generalization, hallucination, bias, and security as key concerns, with arXiv papers showing a shift towards safety, controllability, and multimodality. 17
○    Direct Link: https://arxiv.org/abs/2505.19240
58.    Birdie: Efficient State Space Models through Bidirectional Input Processing and Specialized Pre-Training Objective Mixtures (Posu Chen, Hitu Seth, Parijat Dube, et al., 2024) - A novel training procedure for State Space Models (SSMs) to improve in-context retrieval capabilities, making SSMs more competitive with Transformers on recall-intensive tasks without architectural changes, using bidirectional input and RL-optimized objective mixtures. 20
○    Direct Link: https://arxiv.org/abs/2411.01030
59.    From N-grams to Transformers and Back: A Comparative Behavioral Framework for Bias in Language Models (Debjanee Barua, Gaoge WANG, Elissa M. Redmiles, Michelle Mazurek, Wojtek Palubicki, 2025) - Investigates bias propagation in n-gram vs. Transformer LMs, finding Transformers more robust to contextual bias and offering an interpretable framework to analyze bias propagation mechanisms. 22
○    Direct Link: https://arxiv.org/pdf/2505.12381
60.    Advancing Reasoning in Large Language Models: Promising Methods and Approaches (Avinash Patil, Aryan Jadon, 2025) - A survey reviewing emerging techniques to enhance LLM reasoning, categorized into prompting strategies (CoT, ToT), architectural innovations (retrieval-augmentation, neuro-symbolic), and learning paradigms (fine-tuning, RL). 23
○    Direct Link: https://arxiv.org/abs/2502.03671
61.    Empowering LLMs with Logical Reasoning: A Comprehensive Survey (Fengxiang Cheng, Haoxuan Li, Fenrong Liu, Robert van Rooij, Kun Zhang, Zhouchen Lin, 2025) - A survey focused on logical reasoning in LLMs, covering logical question answering and consistency, with a taxonomy of methods (solver-aided, prompting, pretraining/fine-tuning) and benchmarks. 25
○    Direct Link: https://arxiv.org/abs/2502.15652
62.    Cross-Images Contrastive Decoding: Precise, Lossless Suppression of Language Priors in Large Vision-Language Models (Jianfei Zhao, Feng Zhang, Xin Sun, Chong Feng, 2025) - Introduces CICD, a training-free method to reduce hallucinations in LVLMs by using different images for negative contexts, selectively suppressing detrimental language priors while preserving essential ones. 27
○    Direct Link: https://arxiv.org/abs/2505.10634
63.    Mitigating Hallucinations in Large Vision-Language Models via Summary-Guided Decoding (Kyungmin Min, Minbeom Kim, Kang-il Lee, Dongryeol Lee, Kyomin Jung, 2025) - Proposes SumGD, a decoding strategy to mitigate LVLM hallucinations by encouraging focus on image information through context summarization and selective control of image-related POS token generation. 30
○    Direct Link: https://aclanthology.org/2025.findings-naacl.235.pdf
64.    ConKE: Conceptualization-Augmented Knowledge Editing in Large Language Models for Commonsense Reasoning (Liyu Zhang, Weiqi Wang, Tianqing Fang, Yangqiu Song, 2025) - A knowledge editing framework for commonsense knowledge in LLMs, using an automated verifier to find errors and integrating conceptualization/instantiation for more generalizable edits. 31
○    Direct Link:(https://github.com/HKUST-KnowComp/ConKE) (Code), https://arxiv.org/abs/2412.11418 (Paper)
65.    Multimodal Large Language Models Can Significantly Advance Scientific Reasoning (Jiahua Dong, Zhibin Gou, Yifu Geng, et al., 2025) - A position paper arguing MLLMs can advance scientific reasoning by integrating diverse data types, reviewing current applications, challenges (open-source MLLMs lagging GPT-4o), and future steps. 34
○    Direct Link: https://arxiv.org/pdf/2502.02871
66.    Large Multimodal Models for Low-Resource Languages: A Survey (Mohamed Abdalla, Muhammad Abdul-Mageed, et al., 2025) - Surveys LMMs for low-resource languages (106 studies, 75 languages), highlighting the concentration on high-resource languages and discussing challenges/techniques for LR languages, where text-image pairings are most common. 35
○    Direct Link: https://arxiv.org/pdf/2502.05568
67.    Quantizing Large Language Models for Code Generation: A Differentiated Replication (Alessandro Giagnorio, Antonio Mastropaolo, Saima Afrin, Massimiliano Di Penta, Gabriele Bavota, 2025) - A replication study on quantizing LLMs for code generation (up to 34B parameters, down to 2-4 bits), finding 4-bit precision reduces memory by ~70% with limited performance impact; code-specific calibration helps for 2-3 bits. 36
○    Direct Link: https://doi.org/10.5281/zenodo.13752774 (Replication package), https://arxiv.org/abs/2503.07103 (Paper)
68.    When Reasoning Meets Compression: Benchmarking Compressed Large Reasoning Models on Complex Reasoning Tasks (Nan Zhang, Yusen Zhang, Prasenjit Mitra, Rui Zhang, 2025) - Benchmarks compressed DeepSeek-R1 (quantization, distillation, pruning) on complex reasoning tasks, finding parameter count impacts knowledge memorization more than reasoning, and shorter outputs generally perform better. 38
○    Direct Link: https://arxiv.org/abs/2504.02010
Recent Innovations in Computer Vision (Post-2023)
69.    ECViT: Efficient Convolutional Vision Transformer with Local-Attention and Multi-scale Stages (Zhoujie Qian, 2025) - A hybrid CNN-Transformer architecture (ECViT) that introduces CNN inductive biases (locality, translation invariance) and uses local attention and a pyramid structure for efficient multi-scale feature extraction, balancing performance and computational cost. 40
○    Direct Link: https://arxiv.org/abs/2504.14825
70.    UniViTAR: Unified Vision Transformer with Native Resolution (Limeng Qiao, et al., 2025) - A ViT-based foundation model family (UniViTAR) designed to process images/videos at native resolutions and dynamic aspect ratios, incorporating LLM-inspired upgrades (2D RoPE, SwiGLU, RMSNorm) and progressive training. 43
○    Direct Link: https://arxiv.org/abs/2504.01792
71.    Generative Physical AI in Vision: A Survey (Daochang Liu, Junyu Zhang, Anh-Dung Dinh, Eunbyung Park, Shichao Zhang, Ajmal Mian, Mubarak Shah, Chang Xu, 2025) - A survey on physics-aware generative models in CV, which aim for physically plausible content by incorporating physical knowledge explicitly (simulation) or implicitly (learning from data), towards "world simulators." 46
○    Direct Link: https://arxiv.org/abs/2501.10928 (Summary: https://tinyurl.com/Physics-Aware-Generation)
72.    VBench-2.0: Advancing Video Generation Benchmark Suite for Intrinsic Faithfulness (Large collaborative effort, 2025) - A benchmark suite for advanced video generation models (e.g., Sora, Kling, Veo 2) focusing on "intrinsic faithfulness" across five dimensions: Human Fidelity, Controllability, Creativity, Physics, and Commonsense. 47
○    Direct Link: https://arxiv.org/abs/2503.21755
73.    Controllable 3D Outdoor Scene Generation via Scene Graphs (Yiran Xing, Zhaoxiang Cai, et al., 2025) - Proposes generating controllable outdoor 3D scenes using scene graphs as an intuitive control format, with an interactive system transforming sparse scene graphs to dense BEV Embedding Maps to guide a conditional diffusion model. 10
○    Direct Link: https://arxiv.org/abs/2503.07152
74.    Escaping the Big Data Paradigm in Self-Supervised Representation Learning from Images (Carlos Velez-García, Miguel Cazorla, Jorge Pomares, 2025) - Introduces SCOTT (Sparse Convolutional Tokenizer for Transformers) and MIM-JEPA (Masked Image Modeling with Joint-Embedding Predictive Architecture) to train ViTs with SSL on much smaller datasets without external pretraining, aiming for more accessible SSL. 53
○    Direct Link: https://github.com/inescopresearch/scott (Code), https://arxiv.org/abs/2502.18056 (Paper)
75.    UpStep: Unsupervised Parameter-efficient Source-free Post-pretraining (Ulas Gul, Oguz Kaan Yüksel, et al., 2025) - An unsupervised, parameter-efficient, source-free post-pretraining method (UpStep) to adapt pretrained visual models to unlabeled target domains using SSL and "center vector regularization" (CVR) to minimize catastrophic forgetting and reduce computational costs. 55
○    Direct Link: https://www.arxiv.org/pdf/2502.21313 (Paper PDF)
Recent Developments in Reinforcement Learning (Post-2023)
76.    A Survey on Explainable Deep Reinforcement Learning (Zelei Cheng, Jiahao Yu, Xinyu Xing, 2025) - A survey on XRL, categorizing explanation techniques (feature, state, dataset, model-level), evaluation frameworks, and exploring XRL's role in policy refinement, robustness, security, and its integration with LLMs/RLHF. 56
○    Direct Link: https://arxiv.org/abs/2502.06869
77.    Video-Enhanced Offline RL (VeoRL): A Model-Based Approach (Yecheng Moon, et al., 2025) - A model-based offline RL method (VeoRL) that constructs an interactive world model enhanced with diverse, unlabeled online video data, using a hierarchical world model to predict state evolution and environmental feedback. 14
○    Direct Link: https://arxiv.org/abs/2505.06482
78.    Variational OOD State Correction for Offline Reinforcement Learning (Zicheng Zhang, et al., 2025) - Introduces Density-Aware Safety Perception (DASP) for OOD state correction in offline RL, encouraging agents to prioritize actions leading to higher data density outcomes within a variational framework. 14
○    Direct Link: https://arxiv.org/abs/2505.00503
79.    Addressing Rotational Learning Dynamics in Multi-Agent Reinforcement Learning (Di-An Jan, et al., 2025) - Tackles instability in MARL from "rotational optimization dynamics" by reframing MARL using Variational Inequalities (VIs) and proposing LA-MARL and EG-MARL to integrate gradient-based VI methods into MARL algorithms. 2
○    Direct Link: https://arxiv.org/abs/2410.07976
80.    Offline Multi-agent Reinforcement Learning via Score Decomposition (Lingheng Meng, et al., 2025) - Addresses offline MARL challenges (distributional shifts, high-dimensional action spaces, coordination diversity) with a two-stage framework using a diffusion model to capture joint behavior policy and sequential score function decomposition for regularization. 2
○    Direct Link: https://arxiv.org/abs/2505.05968
81.    A Shared Low-Rank Adaptation Approach to Personalized RLHF (Renpu Liu, Peng Wang, Donghao Li, Cong Shen, Jing Yang, 2025) - Introduces P-ShareLoRA for personalized RLHF, leveraging shared LoRA components to efficiently learn personalized reward functions, addressing homogeneous human preference assumptions in standard RLHF. 4
○    Direct Link: https://arxiv.org/abs/2503.19201
82.    MA-RLHF: Reinforcement Learning from Human Feedback with Macro Actions (Yekun Chai, Haoran Sun, Huang Fang, Shuohuan Wang, Yu Sun, Hua Wu, 2025) - Proposes MA-RLHF to improve token-level RLHF efficiency for long text generation by incorporating "macro-actions" (token sequences or higher-level constructs) to address credit assignment problems. 4
○    Direct Link:(https://github.com/ernie-research/MA-RLHF) (Code), https://arxiv.org/abs/2410.02743 (Paper)
83.    CORL: Clean Offline Reinforcement Learning (Authors of CORL library, presented at NeurIPS 2023) - An open-source library providing benchmarked single-file implementations of deep offline and offline-to-online RL algorithms, emphasizing simplicity and experiment tracking. 70
○    Direct Link: (NeurIPS 2023 proceedings/CORL GitHub)
Recent Progress in General ML Theory and Optimization (Post-2023)
84.    ZeroFlow: Overcoming Catastrophic Forgetting is Easier than You Think (Tao Feng, Wei Li, Didi Zhu, Hangjie Yuan, Wendi Zheng, Dan Zhang, Jie Tang, 2025) - Introduces ZeroFlow, the first benchmark for evaluating gradient-free (zeroth-order) optimization algorithms for continual learning, finding forward passes alone can mitigate catastrophic forgetting and revealing new optimization principles. 71
○    Direct Link: https://arxiv.org/abs/2501.01045
85.    LORENZA: Enhancing Generalization in Low-Rank Gradient LLM Training and Fine-Tuning via Efficient Zeroth-Order Adaptive SAM Optimization (Yehonathan Refael, Iftach Arbel, Ofir Lindenbaum, Tom Tirer, 2025) - Proposes AdaZo-SAM (Adam + SAM with single gradient computation via zeroth-order estimation) and LORENZA (memory-efficient AdaZo-SAM with adaptive low-rank gradient updates) to improve generalization for LLM PEFT and pre-training. 15
○    Direct Link: https://arxiv.org/abs/2502.19571
86.    Deep Multi-Task Learning Has Low Amortized Intrinsic Dimensionality (Hossein Zakerinia, et al., 2025) - Confirms deep MTL models learn within a low intrinsic dimensional subspace and introduces a method to parameterize them directly in this space, leading to the first non-vacuous generalization bounds for deep MTL using weight compression and PAC-Bayesian reasoning. 10
○    Direct Link: https://arxiv.org/abs/2501.19067
87.    Survey on Generalization Theory for Graph Neural Networks (Kajetan Schweighofer, et al., 2025) - Systematically reviews literature on the generalization abilities of Message-Passing Neural Networks (MPNNs), analyzing strengths/limitations of theoretical studies using VC dimension, Rademacher complexity, stability, PAC-Bayesian bounds, etc. 10
○    Direct Link: https://arxiv.org/pdf/2503.15650
88.    Ferret: Federated Full-Parameter Tuning at Scale for Large Language Models (Yao Shu, Wenyang Hu, See-Kiong Ng, Bryan Kian Hsiang Low, Fei Richard Yu, 2024) - Proposes Ferret, the first first-order FL method using shared randomness for scalable full-parameter LLM tuning, balancing accuracy, efficiency, communication overhead, and convergence. 10
○    Direct Link: https://github.com/allen4747/Ferret (Code), https://arxiv.org/abs/2409.06277 (Paper)
89.    FedDDL: Federated Deconfounding and Debiasing Learning for Out-of-Distribution Generalization (Qi Zhuang, Ming-Chang Lee, Han Yu, et al., 2025) - Addresses attribute bias in FL causing poor OOD generalization, proposing FedDDL with a Disentangled Effect Calibration (DEC) module to decouple background/object features and generate counterfactuals for robust causal feature learning. 84
○    Direct Link: https://arxiv.org/abs/2505.04979 (Paper)
90.    FLTG: Byzantine-Robust Federated Learning via Angle-Based Defense and Non-IID-Aware Weighting (Jiahao Wang, et al., 2025) - A Byzantine-robust FL aggregation algorithm (FLTG) using angle-based defense (cosine similarity with server's clean dataset) to filter malicious updates, with dynamic reference selection and non-IID-aware weighting. 86
○    Direct Link: https://arxiv.org/abs/2505.12851
91.    Communication-efficient Vertical Federated Learning via Compressed Error Feedback (Pedro Valdeira, João Xavier, Cláudia Soares, Yuejie Chi, 2024) - Introduces EF-VFL, an error feedback compressed vertical FL method for split neural networks, using lossy compression and error feedback to maintain convergence, supporting private labels. 87
○    Direct Link: https://github.com/pedromvaldeira/EF-VFL (Code), https://arxiv.org/abs/2406.14420 (Paper)
92.    A Statistical Case Against Empirical Human-AI Alignment (Julian Rodemann, Esteban Garces Arias, Christoph Luther, Christoph Jansen, Thomas Augustin, 2025) - Critiques naive empirical human-AI alignment (especially forward/a priori), arguing it can introduce statistical biases, and advocates for prescriptive alignment and a posteriori empirical alignment. 4
○    Direct Link: https://arxiv.org/abs/2502.14581
________________________________________I. Recent Advances in Foundational Model Architectures (Post-2023)
A. Vision Transformers (ViT): New Frontiers in Efficiency and Adaptability
93.    UniViTAR: Unified Vision Transformer with Native Resolution (Limeng Qiao, et al., 2025) - A ViT-based foundation model family (UniViTAR) designed to process images/videos at native resolutions and dynamic aspect ratios, incorporating LLM-inspired upgrades (2D RoPE, SwiGLU, RMSNorm) and progressive training. Domain: CV (Vision Transformer).
○    Direct Link: https://arxiv.org/abs/2504.01792
94.    SCHEME: Scalable CHannEl MixEr for Vision Transformers (Apostolos Modas et al., 2023) - Introduces SCHEME, a scalable channel mixer for ViTs, using a block diagonal MLP and a discardable channel attention branch for training, improving accuracy/latency trade-offs. Domain: CV (Vision Transformer).
○    Direct Link: https://arxiv.org/abs/2312.00412
95.    An Exploratory Approach Towards Investigating and Explaining Vision Transformer and Transfer Learning for Brain Disease Detection (Md. Al Momin Rifat et al., 2023) - Compares ViT and Transfer Learning (VGG16, ResNet50V2 etc.) for brain disease classification from MRI, with ViT achieving 94.39% accuracy. Employs XAI methods (GradCAM etc.) for interpretability. Domain: CV (Medical Imaging, ViT).
○    Direct Link: https://arxiv.org/abs/2305.16039
96.    Vision Foundation Models in Medical Image Analysis: Advances and Challenges (Pengchen Liang, Bin Pu, Haishan Huang, Yiwei Li, Hualiang Wang, Weibo Ma, Qing Chang, 2025) - Reviews adapting Vision Foundation Models (ViTs, SAM) for medical image segmentation, focusing on domain adaptation, model compression, federated learning, and challenges like small datasets. Domain: CV (Medical Imaging, VFMs).
○    Direct Link: https://arxiv.org/abs/2502.14584
B. Transformer Core: Enhancing Capacity and Rethinking Self-Attention
97.    You Do Not Fully Utilize Transformer's Representation Capacity
